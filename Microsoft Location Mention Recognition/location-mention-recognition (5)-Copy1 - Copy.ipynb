{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer\n",
    "import re\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet_id', 'text', 'location_mentions'],\n",
       "        num_rows: 14392\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=load_dataset('json', data_files='train.json')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet_id', 'text', 'location_mentions'],\n",
       "        num_rows: 2056\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val=load_dataset('json', data_files='val.json')\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test=load_dataset('json', data_files='/kaggle/input/lmr-dataset1/test.json')\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1061497252806414336</td>\n",
       "      <td>Please read below!! Another devastating fire h...</td>\n",
       "      <td>[{'end_offset': 72, 'start_offset': 53, 'text'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1061165982855634944</td>\n",
       "      <td>Celebrities evacuate California as wildfires rage</td>\n",
       "      <td>[{'end_offset': 31, 'start_offset': 21, 'text'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1064248785914732544</td>\n",
       "      <td>Grab one of these and HELP victims of Californ...</td>\n",
       "      <td>[{'end_offset': 48, 'start_offset': 38, 'text'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1061160485364613121</td>\n",
       "      <td>the camp fire in Paradise California is growin...</td>\n",
       "      <td>[{'end_offset': 25, 'start_offset': 17, 'text'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1066508972650311682</td>\n",
       "      <td>The area of documented destruction in the #Cam...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1061497252806414336  Please read below!! Another devastating fire h...   \n",
       "1  1061165982855634944  Celebrities evacuate California as wildfires rage   \n",
       "2  1064248785914732544  Grab one of these and HELP victims of Californ...   \n",
       "3  1061160485364613121  the camp fire in Paradise California is growin...   \n",
       "4  1066508972650311682  The area of documented destruction in the #Cam...   \n",
       "\n",
       "                                   location_mentions  \n",
       "0  [{'end_offset': 72, 'start_offset': 53, 'text'...  \n",
       "1  [{'end_offset': 31, 'start_offset': 21, 'text'...  \n",
       "2  [{'end_offset': 48, 'start_offset': 38, 'text'...  \n",
       "3  [{'end_offset': 25, 'start_offset': 17, 'text'...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train['train'].to_pandas()\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1065347154078773250</td>\n",
       "      <td>If youre looking for legitimate relief organiz...</td>\n",
       "      <td>[{'end_offset': 85, 'start_offset': 83, 'text'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1065256190727282689</td>\n",
       "      <td>Like so many things, the destruction of Califo...</td>\n",
       "      <td>[{'end_offset': 51, 'start_offset': 40, 'text'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1066000788177211393</td>\n",
       "      <td>Officials say 563 people are still unaccounted...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1064315981365862400</td>\n",
       "      <td>The wait for Paradise: From young to old, evac...</td>\n",
       "      <td>[{'end_offset': 191, 'start_offset': 186, 'tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1067241739843325952</td>\n",
       "      <td>BREAKING: Camp Fire death toll increases to 88...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1065347154078773250  If youre looking for legitimate relief organiz...   \n",
       "1  1065256190727282689  Like so many things, the destruction of Califo...   \n",
       "2  1066000788177211393  Officials say 563 people are still unaccounted...   \n",
       "3  1064315981365862400  The wait for Paradise: From young to old, evac...   \n",
       "4  1067241739843325952  BREAKING: Camp Fire death toll increases to 88...   \n",
       "\n",
       "                                   location_mentions  \n",
       "0  [{'end_offset': 85, 'start_offset': 83, 'text'...  \n",
       "1  [{'end_offset': 51, 'start_offset': 40, 'text'...  \n",
       "2                                                 []  \n",
       "3  [{'end_offset': 191, 'start_offset': 186, 'tex...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = val['train'].to_pandas()\n",
    "val_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Please read below!! Another devastating fire has hit Northern California, people need help, whatever you can give, or anyway you can help, please doá½¤F!!',\n",
       " 'If youre looking for legitimate relief organizations to help those affected by the CA fires, I found this link: How to Help Those Affected by California Wildfires - Consumer Reports')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirming whether the empty locations mentions have no location mentioned in the text\n",
    "train_df.text[0],val_df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the missing values\n",
    "train_df= train_df[train_df['location_mentions'].apply(lambda x: len(x) > 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10366"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1483"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df= val_df[val_df['location_mentions'].apply(lambda x: len(x) > 0)]\n",
    "len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Celebrities evacuate California as wildfires rage',\n",
       " array([{'end_offset': 31, 'start_offset': 21, 'text': 'California'}],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text[1],train_df.location_mentions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort the locations\n",
    "def sort_locations(locations):\n",
    "    # Sort the list of dictionaries by the 'text' key\n",
    "    return sorted(locations, key=lambda x: x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['location_mentions'] = train_df['location_mentions'].apply(sort_locations)\n",
    "#len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_df['location_mentions'] = val_df['location_mentions'].apply(sort_locations)\n",
    "#len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'end_offset': 25, 'start_offset': 17, 'text': 'Paradise'},\n",
       "       {'end_offset': 36, 'start_offset': 26, 'text': 'California'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.location_mentions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_df, val_df], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11849"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1061497252806414336</td>\n",
       "      <td>Please read below!! Another devastating fire h...</td>\n",
       "      <td>[{'end_offset': 72, 'start_offset': 53, 'text'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1061165982855634944</td>\n",
       "      <td>Celebrities evacuate California as wildfires rage</td>\n",
       "      <td>[{'end_offset': 31, 'start_offset': 21, 'text'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1061497252806414336  Please read below!! Another devastating fire h...   \n",
       "1  1061165982855634944  Celebrities evacuate California as wildfires rage   \n",
       "\n",
       "                                   location_mentions  \n",
       "0  [{'end_offset': 72, 'start_offset': 53, 'text'...  \n",
       "1  [{'end_offset': 31, 'start_offset': 21, 'text'...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>721576641936408576</td>\n",
       "      <td>car &amp; Overpass #EARTHQUAKE in Ecuador upgraded...</td>\n",
       "      <td>[{'end_offset': 37, 'start_offset': 30, 'text'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>908449017134231553</td>\n",
       "      <td>7th grade. Learning about earthquakes with Goo...</td>\n",
       "      <td>[{'end_offset': 101, 'start_offset': 96, 'text...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text  \\\n",
       "0  721576641936408576  car & Overpass #EARTHQUAKE in Ecuador upgraded...   \n",
       "1  908449017134231553  7th grade. Learning about earthquakes with Goo...   \n",
       "\n",
       "                                   location_mentions  \n",
       "0  [{'end_offset': 37, 'start_offset': 30, 'text'...  \n",
       "1  [{'end_offset': 101, 'start_offset': 96, 'text...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data.sample(frac=1).reset_index(drop=True)\n",
    "data1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAW: Aerial view of flooding damage in Nebraska  via @YouTube"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp('RAW: Aerial view of flooding damage in Nebraska  via @YouTube')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RAW: Aerial view of flooding damage in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nebraska\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "  via @YouTube</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert dataframes to dict\n",
    "train_data = data1.to_dict(orient='records')\n",
    "val_data = val_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tweet_id': '721576641936408576',\n",
       "  'text': 'car & Overpass #EARTHQUAKE in Ecuador upgraded to 7.8 magnituden .2016 #ManabÃ­ #ecuador #BREAKING #earthquake',\n",
       "  'location_mentions': array([{'end_offset': 37, 'start_offset': 30, 'text': 'Ecuador'},\n",
       "         {'end_offset': 78, 'start_offset': 72, 'text': 'ManabÃ­'},\n",
       "         {'end_offset': 87, 'start_offset': 80, 'text': 'ecuador'}],\n",
       "        dtype=object)},\n",
       " {'tweet_id': '908449017134231553',\n",
       "  'text': '7th grade. Learning about earthquakes with Google Cardboards and touring earthquake damage from Japan and Nepal. #rockdalepride',\n",
       "  'location_mentions': array([{'end_offset': 101, 'start_offset': 96, 'text': 'Japan'},\n",
       "         {'end_offset': 111, 'start_offset': 106, 'text': 'Nepal'}],\n",
       "        dtype=object)}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: car & Overpass #EARTHQUAKE in Ecuador upgraded to 7.8 magnituden .2016 #ManabÃ­ #ecuador #BREAKING #earthquake\n",
      "Extracted Locations: ['Ecuador', 'ManabÃ­']\n",
      "Text: 7th grade. Learning about earthquakes with Google Cardboards and touring earthquake damage from Japan and Nepal. #rockdalepride\n",
      "Extracted Locations: ['Japan', 'Nepal']\n",
      "Text: 1news reports that TSB Arena & BNZ centre on the waterfront has sustained most damage #Wellington #eqnz\n",
      "Extracted Locations: ['Wellington']\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = train_data[0:3]\n",
    "\n",
    "# Extract location mentions\n",
    "for tweet in data:\n",
    "    text = tweet['text']\n",
    "    doc = nlp(text)\n",
    "    extracted_locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Extracted Locations: {extracted_locations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some locations are not recognized as GPE entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet_id': '1065347154078773250',\n",
       " 'text': 'If youre looking for legitimate relief organizations to help those affected by the CA fires, I found this link: How to Help Those Affected by California Wildfires - Consumer Reports',\n",
       " 'location_mentions': array([{'end_offset': 85, 'start_offset': 83, 'text': 'CA'},\n",
       "        {'end_offset': 152, 'start_offset': 142, 'text': 'California'}],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'car & Overpass #EARTHQUAKE in Ecuador upgraded to 7.8 magnituden .2016 #ManabÃ­ #ecuador #BREAKING #earthquake',\n",
       "  'entities': [(30, 37, 'LOCATION', 'Ecuador'),\n",
       "   (72, 78, 'LOCATION', 'ManabÃ­'),\n",
       "   (80, 87, 'LOCATION', 'ecuador')]},\n",
       " {'text': '7th grade. Learning about earthquakes with Google Cardboards and touring earthquake damage from Japan and Nepal. #rockdalepride',\n",
       "  'entities': [(96, 101, 'LOCATION', 'Japan'),\n",
       "   (106, 111, 'LOCATION', 'Nepal')]},\n",
       " {'text': '1news reports that TSB Arena & BNZ centre on the waterfront has sustained most damage #Wellington #eqnz',\n",
       "  'entities': [(87, 97, 'LOCATION', 'Wellington')]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds=[]\n",
    "for data in train_data:\n",
    "    temp_dict={}\n",
    "    temp_dict['text'] = data['text']\n",
    "    temp_dict['entities'] = []\n",
    "    for annotation in data['location_mentions']:\n",
    "        start = annotation['start_offset']\n",
    "        end = annotation['end_offset']\n",
    "        label = \"LOCATION\"\n",
    "        text = annotation['text']\n",
    "        temp_dict['entities'].append((start,end,label,text))\n",
    "    train_ds.append(temp_dict)\n",
    "\n",
    "train_ds[0:3]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'If youre looking for legitimate relief organizations to help those affected by the CA fires, I found this link: How to Help Those Affected by California Wildfires - Consumer Reports',\n",
       "  'entities': [(83, 85, 'LOCATION', 'CA'),\n",
       "   (142, 152, 'LOCATION', 'California')]},\n",
       " {'text': 'Like so many things, the destruction of Californias massive Camp fire was less natural, more man made. The Camp fire burned homes but left trees standing.',\n",
       "  'entities': [(40, 51, 'LOCATION', 'Californias')]},\n",
       " {'text': 'The wait for Paradise: From young to old, evacuees displaced by the #campfire endure another restless & cold night in their cars, container trucks & tents, at the Walmart parking lot in Chico, Calif.',\n",
       "  'entities': [(186, 191, 'LOCATION', 'Chico'),\n",
       "   (193, 199, 'LOCATION', 'Calif.')]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds=[]\n",
    "for data in val_data:\n",
    "    temp_dict={}\n",
    "    temp_dict['text'] = data['text']\n",
    "    temp_dict['entities'] = []\n",
    "    for annotation in data['location_mentions']:\n",
    "        start = annotation['start_offset']\n",
    "        end = annotation['end_offset']\n",
    "        label = \"LOCATION\"\n",
    "        text = annotation['text']\n",
    "        temp_dict['entities'].append((start,end,label,text))\n",
    "    val_ds.append(temp_dict)\n",
    "\n",
    "val_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(text, entities):\n",
    "    tokenized_inputs = tokenizer(text, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "    labels = [0] * len(tokenized_inputs[\"input_ids\"])  # Default to O label\n",
    "    for start, end, label, text in entities:\n",
    "        # Get the tokens that fall within the entity span\n",
    "        for i, (offset_start, offset_end) in enumerate(tokenized_inputs[\"offset_mapping\"]):\n",
    "            if offset_start == start:\n",
    "                labels[i] = 1  # B-LOCATION\n",
    "            elif offset_start > start and offset_end <= end:\n",
    "                labels[i] = 2  # I-LOCATION\n",
    "\n",
    "    # Mask special tokens ([CLS], [SEP]) with -100\n",
    "    labels[0] = -100  # [CLS]\n",
    "    labels[-1] = -100  # [SEP]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds1 = []\n",
    "for example in train_ds:\n",
    "    tokenized_example = tokenize_and_align_labels(example['text'], example['entities'])\n",
    "    train_ds1.append(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'MALDIVES DONATES USD 50,000 FOR KERALA RELIEF Token contribution in solidarity with the people of Indiaâ Maldives Presidentâs Office. #KeralaSOS #KeralaFloods @MDVinIND @SushmaSwaraj',\n",
       " 'entities': [(0, 8, 'LOCATION', 'MALDIVES'),\n",
       "  (32, 38, 'LOCATION', 'KERALA'),\n",
       "  (98, 103, 'LOCATION', 'India'),\n",
       "  (105, 113, 'LOCATION', 'Maldives')]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds1 = []\n",
    "for example in val_ds:\n",
    "    tokenized_example = tokenize_and_align_labels(example['text'], example['entities'])\n",
    "    val_ds1.append(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mappings\n",
    "id2label = {\n",
    "    0: 'O',           \n",
    "    1: 'B-LOCATION',  \n",
    "    2: 'I-LOCATION', \n",
    "    -100: 'IGNORE'    \n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "   'bert-base-cased',\n",
    "    num_labels=len(label2id), \n",
    "    label2id=label2id,         \n",
    "    id2label=id2label           \n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    flattened_labels = labels.flatten()\n",
    "    flattened_preds = preds.flatten()\n",
    "    valid_indices = flattened_labels != -100\n",
    "    filtered_labels = flattened_labels[valid_indices]\n",
    "    filtered_preds = flattened_preds[valid_indices]\n",
    "    \n",
    "    # Handle case where there are no valid labels after filtering\n",
    "    if len(filtered_labels) == 0:\n",
    "        return {\n",
    "            'accuracy': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0\n",
    "        }\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(filtered_labels, filtered_preds, average='weighted')\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc = accuracy_score(filtered_labels, filtered_preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer,label_pad_token_id=-100)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ð¤ Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=70,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=70,\n",
    "    save_steps=70,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=6e-5,\n",
    "    lr_scheduler_type='cosine',\n",
    "    gradient_accumulation_steps=5,  \n",
    "    fp16=False, \n",
    "    report_to=[\"none\"],\n",
    "    max_grad_norm=1.0,\n",
    "    seed=42\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds1,\n",
    "    eval_dataset=val_ds1,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1422' max='1422' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1422/1422 5:45:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>0.978314</td>\n",
       "      <td>0.977488</td>\n",
       "      <td>0.977652</td>\n",
       "      <td>0.978314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.048312</td>\n",
       "      <td>0.982278</td>\n",
       "      <td>0.982239</td>\n",
       "      <td>0.982221</td>\n",
       "      <td>0.982278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.042034</td>\n",
       "      <td>0.984410</td>\n",
       "      <td>0.984332</td>\n",
       "      <td>0.984278</td>\n",
       "      <td>0.984410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.987147</td>\n",
       "      <td>0.987221</td>\n",
       "      <td>0.987326</td>\n",
       "      <td>0.987147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.036026</td>\n",
       "      <td>0.986335</td>\n",
       "      <td>0.986060</td>\n",
       "      <td>0.986018</td>\n",
       "      <td>0.986335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.033773</td>\n",
       "      <td>0.987940</td>\n",
       "      <td>0.987922</td>\n",
       "      <td>0.987913</td>\n",
       "      <td>0.987940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.029237</td>\n",
       "      <td>0.990582</td>\n",
       "      <td>0.990572</td>\n",
       "      <td>0.990574</td>\n",
       "      <td>0.990582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.025263</td>\n",
       "      <td>0.991394</td>\n",
       "      <td>0.991365</td>\n",
       "      <td>0.991345</td>\n",
       "      <td>0.991394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.023748</td>\n",
       "      <td>0.992035</td>\n",
       "      <td>0.992077</td>\n",
       "      <td>0.992138</td>\n",
       "      <td>0.992035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.020821</td>\n",
       "      <td>0.992451</td>\n",
       "      <td>0.992494</td>\n",
       "      <td>0.992566</td>\n",
       "      <td>0.992451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>0.993262</td>\n",
       "      <td>0.993320</td>\n",
       "      <td>0.993426</td>\n",
       "      <td>0.993262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.017718</td>\n",
       "      <td>0.994055</td>\n",
       "      <td>0.994047</td>\n",
       "      <td>0.994043</td>\n",
       "      <td>0.994055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.016154</td>\n",
       "      <td>0.994923</td>\n",
       "      <td>0.994955</td>\n",
       "      <td>0.995013</td>\n",
       "      <td>0.994923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.013553</td>\n",
       "      <td>0.995263</td>\n",
       "      <td>0.995284</td>\n",
       "      <td>0.995318</td>\n",
       "      <td>0.995263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.013010</td>\n",
       "      <td>0.995508</td>\n",
       "      <td>0.995522</td>\n",
       "      <td>0.995543</td>\n",
       "      <td>0.995508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.012335</td>\n",
       "      <td>0.995735</td>\n",
       "      <td>0.995753</td>\n",
       "      <td>0.995784</td>\n",
       "      <td>0.995735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.011790</td>\n",
       "      <td>0.995999</td>\n",
       "      <td>0.996010</td>\n",
       "      <td>0.996026</td>\n",
       "      <td>0.995999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.996093</td>\n",
       "      <td>0.996110</td>\n",
       "      <td>0.996137</td>\n",
       "      <td>0.996093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.011362</td>\n",
       "      <td>0.996074</td>\n",
       "      <td>0.996091</td>\n",
       "      <td>0.996120</td>\n",
       "      <td>0.996074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>0.996131</td>\n",
       "      <td>0.996147</td>\n",
       "      <td>0.996175</td>\n",
       "      <td>0.996131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1422, training_loss=0.03800686841775596, metrics={'train_runtime': 20737.5684, 'train_samples_per_second': 1.714, 'train_steps_per_second': 0.069, 'total_flos': 1069917799001976.0, 'train_loss': 0.03800686841775596, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 02:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011307409964501858, 'eval_accuracy': 0.9961309074437567, 'eval_f1': 0.9961473056876373, 'eval_precision': 0.9961753184205558, 'eval_recall': 0.9961309074437567, 'eval_runtime': 173.2398, 'eval_samples_per_second': 8.56, 'eval_steps_per_second': 1.714, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results\\\\tokenizer_config.json',\n",
       " './results\\\\special_tokens_map.json',\n",
       " './results\\\\vocab.txt',\n",
       " './results\\\\added_tokens.json',\n",
       " './results\\\\tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./results')\n",
    "tokenizer.save_pretrained('./results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5604005,
     "sourceId": 9261507,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5655565,
     "sourceId": 9333522,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
